{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"duration":216.743164,"end_time":"2021-02-06T01:18:13.273240","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-02-06T01:14:36.530076","version":"2.1.0"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":75176,"databundleVersionId":8252256,"sourceType":"competition"},{"sourceId":8496210,"sourceType":"datasetVersion","datasetId":5069691},{"sourceId":8503943,"sourceType":"datasetVersion","datasetId":5075508}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VinBigData detectron2 prediction\n\n\n**Following from the training kernel [VinBigData detectron2 train](https://www.kaggle.com/corochann/vinbigdata-detectron2-train), I will try prediction with the `detectron2` trained model**\n\n`detectron2` is one of the famous pytorch object detection library, I will introduce how to use this library to predict bounding boxes with the trained model.\n\n - https://github.com/facebookresearch/detectron2\n\n> Detectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron, and it originates from maskrcnn-benchmark.\n![](https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png)\n\n\n## Version history\n\n2021/1/22: Update to add 2-class filter in troduced in [VinBigData ðŸŒŸ2 Class FilterðŸŒŸ](https://www.kaggle.com/awsaf49/vinbigdata-2-class-filter) by @awsaf49 <br/>\nI also wrote kernel to train 2-class model: [ðŸ“¸VinBigData 2-class classifier complete pipeline](https://www.kaggle.com/corochann/vinbigdata-2-class-classifier-complete-pipeline)\n\n2021/2/6: Updated trained model [vinbigdata-alb-aug-512-cos](https://www.kaggle.com/corochann/vinbigdata-alb-aug-512-cos).<br/>\nUpdated prediction kernel to align training kernel [VinBigData detectron2 train](https://www.kaggle.com/corochann/vinbigdata-detectron2-train), which uses customized augmentation.<br/>\nApply 2-class filter in [ðŸ“¸VinBigData 2-class classifier complete pipeline](https://www.kaggle.com/corochann/vinbigdata-2-class-classifier-complete-pipeline).","metadata":{"papermill":{"duration":0.014923,"end_time":"2021-02-06T01:14:40.700485","exception":false,"start_time":"2021-02-06T01:14:40.685562","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Table of Contents\n\n** [Prediction method implementations](#pred_method)** <br/>\n** [Prediction scripts](#pred_scripts)** <br/>\n** [Apply 2 class filter](#2class)** <br/>\n** [Other kernels](#ref)** <br/>\n\nSince first setup part is same with the training kernel, I skipped listing on ToC.","metadata":{"papermill":{"duration":0.013265,"end_time":"2021-02-06T01:14:40.727572","exception":false,"start_time":"2021-02-06T01:14:40.714307","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Dataset preparation\n\nPreprocessing x-ray image format (dicom) into normal png image format is already done by @xhlulu in the below discussion:\n - [Multiple preprocessed datasets: 256/512/1024px, PNG and JPG, modified and original ratio](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/discussion/207955).\n\nHere I will just use the dataset [VinBigData Chest X-ray Resized PNG (256x256)](https://www.kaggle.com/xhlulu/vinbigdata-chest-xray-resized-png-256x256) to skip the preprocessing and focus on modeling part. Please upvote the dataset as well!","metadata":{"papermill":{"duration":0.013207,"end_time":"2021-02-06T01:14:40.754198","exception":false,"start_time":"2021-02-06T01:14:40.740991","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":8.589343,"end_time":"2021-02-06T01:14:49.357094","exception":false,"start_time":"2021-02-06T01:14:40.767751","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:19:46.140990Z","iopub.execute_input":"2024-05-24T14:19:46.141339Z","iopub.status.idle":"2024-05-24T14:19:56.246494Z","shell.execute_reply.started":"2024-05-24T14:19:46.141309Z","shell.execute_reply":"2024-05-24T14:19:56.245717Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4188802921.py:16: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display, HTML\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"# Installation\n\ndetectron2 is not pre-installed in this kaggle docker, so let's install it. \nWe can follow [installation instruction](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md), we need to know CUDA and pytorch version to install correct `detectron2`.","metadata":{"papermill":{"duration":0.014503,"end_time":"2021-02-06T01:14:49.387151","exception":false,"start_time":"2021-02-06T01:14:49.372648","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.696483,"end_time":"2021-02-06T01:14:50.098363","exception":false,"start_time":"2021-02-06T01:14:49.401880","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:19:56.247977Z","iopub.execute_input":"2024-05-24T14:19:56.248249Z","iopub.status.idle":"2024-05-24T14:19:57.297263Z","shell.execute_reply.started":"2024-05-24T14:19:56.248226Z","shell.execute_reply":"2024-05-24T14:19:57.296101Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Fri May 24 14:19:57 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   46C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.651732,"end_time":"2021-02-06T01:14:50.765678","exception":false,"start_time":"2021-02-06T01:14:50.113946","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:19:57.298837Z","iopub.execute_input":"2024-05-24T14:19:57.299195Z","iopub.status.idle":"2024-05-24T14:19:58.288719Z","shell.execute_reply.started":"2024-05-24T14:19:57.299165Z","shell.execute_reply":"2024-05-24T14:19:58.287780Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ntorch.__version__","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":1.119558,"end_time":"2021-02-06T01:14:51.901633","exception":false,"start_time":"2021-02-06T01:14:50.782075","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:19:58.291034Z","iopub.execute_input":"2024-05-24T14:19:58.291345Z","iopub.status.idle":"2024-05-24T14:19:58.298583Z","shell.execute_reply.started":"2024-05-24T14:19:58.291316Z","shell.execute_reply":"2024-05-24T14:19:58.297663Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'2.1.2'"},"metadata":{}}]},{"cell_type":"markdown","source":"It seems CUDA=10.2 and torch==1.7.0 is used in this kaggle docker image.\n\nSee [installation](https://detectron2.readthedocs.io/tutorials/install.html) for details.","metadata":{"papermill":{"duration":0.01598,"end_time":"2021-02-06T01:14:51.934704","exception":false,"start_time":"2021-02-06T01:14:51.918724","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"pred_method\"></a>\n# Prediction method implementations\n\nBasically we don't need to implement neural network part, `detectron2` already implements famous architectures and provides its pre-trained weights. We can finetune these pre-trained architectures.\n\nThese models are summarized in [MODEL_ZOO.md](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md).\n\nIn this competition, we need object detection model, I will choose [R50-FPN](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml) for this kernel.","metadata":{"papermill":{"duration":0.030375,"end_time":"2021-02-06T01:15:17.747895","exception":false,"start_time":"2021-02-06T01:15:17.717520","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Data preparation\n\n`detectron2` provides high-level API for training custom dataset.\n\nTo define custom dataset, we need to create **list of dict** where each dict contains following:\n\n - file_name: file name of the image.\n - image_id: id of the image, index is used here.\n - height: height of the image.\n - width: width of the image.\n - annotation: This is the ground truth annotation data for object detection, which contains following\n     - bbox: bounding box pixel location with shape (n_boxes, 4)\n     - bbox_mode: `BoxMode.XYXY_ABS` is used here, meaning that absolute value of (xmin, ymin, xmax, ymax) annotation is used in the `bbox`.\n     - category_id: class label id for each bounding box, with shape (n_boxes,)\n\n`get_vinbigdata_dicts` is for train dataset preparation and `get_vinbigdata_dicts_test` is for test dataset preparation.","metadata":{"papermill":{"duration":0.030117,"end_time":"2021-02-06T01:15:17.808496","exception":false,"start_time":"2021-02-06T01:15:17.778379","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pickle\nfrom pathlib import Path\nfrom typing import Optional\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom detectron2.structures import BoxMode\nfrom tqdm import tqdm\n\n\ndef get_vinbigdata_dicts(\n    imgdir: Path,\n    train_df: pd.DataFrame,\n    train_data_type: str = \"original\",\n    use_cache: bool = True,\n    debug: bool = True,\n    target_indices: Optional[np.ndarray] = None,\n    use_class14: bool = False,\n):\n    debug_str = f\"_debug{int(debug)}\"\n    train_data_type_str = f\"_{train_data_type}\"\n    class14_str = f\"_14class{int(use_class14)}\"\n    cache_path = Path(\".\") / f\"dataset_dicts_cache{train_data_type_str}{class14_str}{debug_str}.pkl\"\n    if not use_cache or not cache_path.exists():\n        print(\"Creating data...\")\n        train_meta = pd.read_csv(\"/kaggle/input/amia-public-challenge-2024/img_size.csv\")\n        df_train = pd.read_csv(\"/kaggle/input/amia-public-challenge-2024/train.csv\")\n        \n        \n        \n        train_meta = train_meta[train_meta['image_id'].isin(df_train['image_id'])]\n        \n        \n        if debug:\n            train_meta = train_meta.iloc[:500]  # For debug....\n\n        # Load 1 image to get image size.\n        image_id = \"00JgsY3R0C6VQrT7VDFcoqW2J7dOfULr\"\n        image_path = str(imgdir / f\"{image_id}.png\")\n        image = cv2.imread(image_path)\n        resized_height, resized_width, ch = image.shape\n        print(f\"image shape: {image.shape}\")\n\n        dataset_dicts = []\n        for index, train_meta_row in tqdm(train_meta.iterrows(), total=len(train_meta)):\n            record = {}\n\n            image_id, height, width = train_meta_row.values\n            filename = str(imgdir /  f\"{image_id}.png\")\n            record[\"file_name\"] = filename\n            record[\"image_id\"] = image_id\n            record[\"height\"] = resized_height\n            record[\"width\"] = resized_width\n            objs = []\n            for index2, row in train_df.query(\"image_id == @image_id\").iterrows():\n                # print(row)\n                # print(row[\"class_name\"])\n                # class_name = row[\"class_name\"]\n                class_id = row[\"class_id\"]\n                if class_id == 14:\n                    # It is \"No finding\"\n                    if use_class14:\n                        # Use this No finding class with the bbox covering all image area.\n                        bbox_resized = [0, 0, resized_width, resized_height]\n                        obj = {\n                            \"bbox\": bbox_resized,\n                            \"bbox_mode\": BoxMode.XYXY_ABS,\n                            \"category_id\": class_id,\n                        }\n                        objs.append(obj)\n                    else:\n                        # This annotator does not find anything, skip.\n                        pass\n                else:\n                    # bbox_original = [int(row[\"x_min\"]), int(row[\"y_min\"]), int(row[\"x_max\"]), int(row[\"y_max\"])]\n                    h_ratio = resized_height / height\n                    w_ratio = resized_width / width\n                    bbox_resized = [\n                        float(row[\"x_min\"]) * w_ratio,\n                        float(row[\"y_min\"]) * h_ratio,\n                        float(row[\"x_max\"]) * w_ratio,\n                        float(row[\"y_max\"]) * h_ratio,\n                    ]\n                    obj = {\n                        \"bbox\": bbox_resized,\n                        \"bbox_mode\": BoxMode.XYXY_ABS,\n                        \"category_id\": class_id,\n                    }\n                    objs.append(obj)\n            record[\"annotations\"] = objs\n            dataset_dicts.append(record)\n        with open(cache_path, mode=\"wb\") as f:\n            pickle.dump(dataset_dicts, f)\n\n    print(f\"Load from cache {cache_path}\")\n    with open(cache_path, mode=\"rb\") as f:\n        dataset_dicts = pickle.load(f)\n    if target_indices is not None:\n        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n    return dataset_dicts\n\n\ndef get_vinbigdata_dicts_test(\n    imgdir: Path, test_meta: pd.DataFrame, use_cache: bool = True, debug: bool = True,\n):\n    debug_str = f\"_debug{int(debug)}\"\n    cache_path = Path(\".\") / f\"dataset_dicts_cache_test{debug_str}.pkl\"\n    if not use_cache or not cache_path.exists():\n        print(\"Creating data...\")\n        # test_meta = pd.read_csv(imgdir / \"test_meta.csv\")\n        if debug:\n            test_meta = test_meta.iloc[:500]  # For debug....\n\n        # Load 1 image to get image size.\n        image_id = \"00JgsY3R0C6VQrT7VDFcoqW2J7dOfULr\"\n        image_path = \"/kaggle/input/amia-public-challenge-2024/train/train/00JgsY3R0C6VQrT7VDFcoqW2J7dOfULr.png\"\n        image = cv2.imread(image_path)\n        resized_height, resized_width, ch = image.shape\n        print(f\"image shape: {image.shape}\")\n\n        dataset_dicts = []\n        for index, test_meta_row in tqdm(test_meta.iterrows(), total=len(test_meta)):\n            record = {}\n\n            image_id, height, width = test_meta_row.values\n            filename = str(imgdir / \"test\" / f\"{image_id}.png\")\n            record[\"file_name\"] = filename\n            # record[\"image_id\"] = index\n            record[\"image_id\"] = image_id\n            record[\"height\"] = resized_height\n            record[\"width\"] = resized_width\n            # objs = []\n            # record[\"annotations\"] = objs\n            dataset_dicts.append(record)\n        with open(cache_path, mode=\"wb\") as f:\n            pickle.dump(dataset_dicts, f)\n\n    print(f\"Load from cache {cache_path}\")\n    with open(cache_path, mode=\"rb\") as f:\n        dataset_dicts = pickle.load(f)\n    return dataset_dicts\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.582391,"end_time":"2021-02-06T01:15:18.420967","exception":false,"start_time":"2021-02-06T01:15:17.838576","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:22:05.294222Z","iopub.execute_input":"2024-05-24T14:22:05.294614Z","iopub.status.idle":"2024-05-24T14:22:10.097647Z","shell.execute_reply.started":"2024-05-24T14:22:05.294577Z","shell.execute_reply":"2024-05-24T14:22:10.096897Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Methods for prediction for this competition","metadata":{"papermill":{"duration":0.030425,"end_time":"2021-02-06T01:15:18.482512","exception":false,"start_time":"2021-02-06T01:15:18.452087","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Methods for prediction for this competition\nfrom math import ceil\nfrom typing import Any, Dict, List\n\nimport cv2\nimport detectron2\nimport numpy as np\nfrom numpy import ndarray\nimport pandas as pd\nimport torch\nfrom detectron2 import model_zoo\nfrom detectron2.config import get_cfg\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.utils.visualizer import ColorMode, Visualizer\nfrom tqdm import tqdm\n\n\ndef format_pred(labels: ndarray, boxes: ndarray, scores: ndarray) -> str:\n    pred_strings = []\n    for label, score, bbox in zip(labels, scores, boxes):\n        xmin, ymin, xmax, ymax = bbox.astype(np.int64)\n        pred_strings.append(f\"{label} {score} {xmin} {ymin} {xmax} {ymax}\")\n    return \" \".join(pred_strings)\n\n\ndef predict_batch(predictor: DefaultPredictor, im_list: List[ndarray]) -> List:\n    with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n        inputs_list = []\n        for original_image in im_list:\n            # Apply pre-processing to image.\n            if predictor.input_format == \"RGB\":\n                # whether the model expects BGR inputs or RGB\n                original_image = original_image[:, :, ::-1]\n            height, width = original_image.shape[:2]\n            # Do not apply original augmentation, which is resize.\n            # image = predictor.aug.get_transform(original_image).apply_image(original_image)\n            image = original_image\n            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n            inputs = {\"image\": image, \"height\": height, \"width\": width}\n            inputs_list.append(inputs)\n        predictions = predictor.model(inputs_list)\n        return predictions","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.146868,"end_time":"2021-02-06T01:15:18.660146","exception":false,"start_time":"2021-02-06T01:15:18.513278","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:22:10.100502Z","iopub.execute_input":"2024-05-24T14:22:10.101553Z","iopub.status.idle":"2024-05-24T14:22:10.732168Z","shell.execute_reply.started":"2024-05-24T14:22:10.101523Z","shell.execute_reply":"2024-05-24T14:22:10.731200Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# --- utils ---\nfrom pathlib import Path\nfrom typing import Any, Union\n\nimport yaml\n\n\ndef save_yaml(filepath: Union[str, Path], content: Any, width: int = 120):\n    with open(filepath, \"w\") as f:\n        yaml.dump(content, f, width=width)\n\n\ndef load_yaml(filepath: Union[str, Path]) -> Any:\n    with open(filepath, \"r\") as f:\n        content = yaml.full_load(f)\n    return content\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.040308,"end_time":"2021-02-06T01:15:18.731448","exception":false,"start_time":"2021-02-06T01:15:18.691140","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:22:10.733411Z","iopub.execute_input":"2024-05-24T14:22:10.733744Z","iopub.status.idle":"2024-05-24T14:22:10.740397Z","shell.execute_reply.started":"2024-05-24T14:22:10.733719Z","shell.execute_reply":"2024-05-24T14:22:10.739483Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# --- configs ---\nthing_classes = [\n    \"Aortic enlargement\",\n    \"Atelectasis\",\n    \"Calcification\",\n    \"Cardiomegaly\",\n    \"Consolidation\",\n    \"ILD\",\n    \"Infiltration\",\n    \"Lung Opacity\",\n    \"Nodule/Mass\",\n    \"Other lesion\",\n    \"Pleural effusion\",\n    \"Pleural thickening\",\n    \"Pneumothorax\",\n    \"Pulmonary fibrosis\"\n]\ncategory_name_to_id = {class_name: index for index, class_name in enumerate(thing_classes)}\n","metadata":{"papermill":{"duration":0.038382,"end_time":"2021-02-06T01:15:18.800387","exception":false,"start_time":"2021-02-06T01:15:18.762005","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:22:10.741539Z","iopub.execute_input":"2024-05-24T14:22:10.741808Z","iopub.status.idle":"2024-05-24T14:22:10.762046Z","shell.execute_reply.started":"2024-05-24T14:22:10.741787Z","shell.execute_reply":"2024-05-24T14:22:10.761169Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"This `Flags` class is to manage experiments. I will tune these parameters through the competition to improve model's performance.","metadata":{"papermill":{"duration":0.030436,"end_time":"2021-02-06T01:15:18.861660","exception":false,"start_time":"2021-02-06T01:15:18.831224","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- flags ---\nfrom dataclasses import dataclass, field\nfrom typing import Dict\n\n\n@dataclass\nclass Flags:\n    # General\n    debug: bool = True\n    outdir: str = \"results/det\"\n\n    # Data config\n    imgdir_name: str = \"/kaggle/input/amia-public-challenge-2024/test/test\"\n    split_mode: str = \"all_train\"  # all_train or valid20\n    seed: int = 111\n    train_data_type: str = \"original\"  # original or wbf\n    use_class14: bool = False\n    # Training config\n    iter: int = 10000\n    ims_per_batch: int = 2  # images per batch, this corresponds to \"total batch size\"\n    num_workers: int = 4\n    lr_scheduler_name: str = \"WarmupMultiStepLR\"  # WarmupMultiStepLR (default) or WarmupCosineLR\n    base_lr: float = 0.00025\n    roi_batch_size_per_image: int = 512\n    eval_period: int = 10000\n    aug_kwargs: Dict = field(default_factory=lambda: {})\n\n    def update(self, param_dict: Dict) -> \"Flags\":\n        # Overwrite by `param_dict`\n        for key, value in param_dict.items():\n            if not hasattr(self, key):\n                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n            setattr(self, key, value)\n        return self","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.044023,"end_time":"2021-02-06T01:15:18.936724","exception":false,"start_time":"2021-02-06T01:15:18.892701","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:22:10.763297Z","iopub.execute_input":"2024-05-24T14:22:10.763814Z","iopub.status.idle":"2024-05-24T14:22:10.778762Z","shell.execute_reply.started":"2024-05-24T14:22:10.763783Z","shell.execute_reply":"2024-05-24T14:22:10.777991Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"pred_scripts\"></a>\n# Prediction scripts\n\nNow the methods are ready. Main training scripts starts from here.","metadata":{"papermill":{"duration":0.062839,"end_time":"2021-02-06T01:15:19.030588","exception":false,"start_time":"2021-02-06T01:15:18.967749","status":"completed"},"tags":[]}},{"cell_type":"code","source":"inputdir = Path(\"/kaggle/input\")\ntraineddir = inputdir / \"tunned-model-v1\"\n\n# flags = Flags()\nflags: Flags = Flags().update(load_yaml(str(traineddir/\"tunned_flags.yaml\")))\nflags.imgdir_name = \"amia-public-challenge-2024/test/\"\nprint(\"flags\", flags)\ndebug = flags.debug\n# flags_dict = dataclasses.asdict(flags)\noutdir = Path(flags.outdir)\nos.makedirs(str(outdir), exist_ok=True)\n\n\n# --- Read data ---\ndatadir = inputdir / \"amia-public-challenge-2024\"\nimgdir = inputdir / flags.imgdir_name\n\n# Read in the data CSV files\n# train = pd.read_csv(datadir / \"train.csv\")\ntest_meta = pd.read_csv(\"/kaggle/input/amia-public-challenge-2024/img_size.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/amia-public-challenge-2024/test.csv\")\ntest_meta = test_meta[test_meta['image_id'].isin(df_train['image_id'])]\nsample_submission = pd.read_csv(datadir / \"sample_submission.csv\")","metadata":{"papermill":{"duration":0.089808,"end_time":"2021-02-06T01:15:19.151183","exception":false,"start_time":"2021-02-06T01:15:19.061375","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T14:22:10.779955Z","iopub.execute_input":"2024-05-24T14:22:10.780693Z","iopub.status.idle":"2024-05-24T14:22:10.927054Z","shell.execute_reply.started":"2024-05-24T14:22:10.780667Z","shell.execute_reply":"2024-05-24T14:22:10.926175Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"flags Flags(debug=False, outdir='results/v9', imgdir_name='amia-public-challenge-2024/test/', split_mode='valid20', seed=111, train_data_type='original', use_class14=False, iter=679, ims_per_batch=14, num_workers=4, lr_scheduler_name='WarmupCosineLR', base_lr=0.0023230241019715457, roi_batch_size_per_image=472, eval_period=1000, aug_kwargs={'HorizontalFlip': {'p': 0.5}, 'RandomBrightnessContrast': {'p': 0.5}, 'ShiftScaleRotate': {'p': 0.5, 'rotate_limit': 10, 'scale_limit': 0.15}})\n","output_type":"stream"}]},{"cell_type":"code","source":"cfg = get_cfg()\noriginal_output_dir = cfg.OUTPUT_DIR\ncfg.OUTPUT_DIR = str(outdir)\nprint(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"vinbigdata_train\",)\ncfg.DATASETS.TEST = ()\n# cfg.DATASETS.TEST = (\"vinbigdata_train\",)\n# cfg.TEST.EVAL_PERIOD = 50\ncfg.DATALOADER.NUM_WORKERS = 2\n# Let training initialize from model zoo\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = flags.base_lr  # pick a good LR\ncfg.SOLVER.MAX_ITER = flags.iter\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = flags.roi_batch_size_per_image\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\n# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n\n### --- Inference & Evaluation ---\n# Inference should use the config with parameters that are used in training\n# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n# path to the model we just trained\ncfg.MODEL.WEIGHTS = str(traineddir/\"tunning_v1.pth\")\nprint(\"Original thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)  # 0.05\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15  # set a custom testing threshold\nprint(\"Changed  thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\npredictor = DefaultPredictor(cfg)\n\nDatasetCatalog.register(\n    \"vinbigdata_test\", lambda: get_vinbigdata_dicts_test(imgdir, test_meta, debug=debug)\n)\nMetadataCatalog.get(\"vinbigdata_test\").set(thing_classes=thing_classes)\nmetadata = MetadataCatalog.get(\"vinbigdata_test\")\ndataset_dicts = get_vinbigdata_dicts_test(imgdir, test_meta, debug=debug)\n\nif debug:\n    dataset_dicts = dataset_dicts[:100]\n\nresults_list = []\nindex = 0\nbatch_size = 4\n\nfor i in tqdm(range(ceil(len(dataset_dicts) / batch_size))):\n    inds = list(range(batch_size * i, min(batch_size * (i + 1), len(dataset_dicts))))\n    dataset_dicts_batch = [dataset_dicts[i] for i in inds]\n    im_list = [cv2.imread(d[\"file_name\"]) for d in dataset_dicts_batch]\n    outputs_list = predict_batch(predictor, im_list)\n\n    for im, outputs, d in zip(im_list, outputs_list, dataset_dicts_batch):\n        resized_height, resized_width, ch = im.shape\n        # outputs = predictor(im)\n        if index < 5:\n            # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n            v = Visualizer(\n                im[:, :, ::-1],\n                metadata=metadata,\n                scale=0.5,\n                instance_mode=ColorMode.IMAGE_BW\n                # remove the colors of unsegmented pixels. This option is only available for segmentation models\n            )\n            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n            # cv2_imshow(out.get_image()[:, :, ::-1])\n            cv2.imwrite(str(outdir / f\"pred_{index}.jpg\"), out.get_image()[:, :, ::-1])\n\n        image_id, dim0, dim1 = test_meta.iloc[index].values\n\n        instances = outputs[\"instances\"]\n        if len(instances) == 0:\n            # No finding, let's set 14 1 0 0 1 1x.\n            result = {\"image_id\": image_id, \"PredictionString\": \"14 1.0 0 0 1 1\"}\n        else:\n            # Find some bbox...\n            # print(f\"index={index}, find {len(instances)} bbox.\")\n            fields: Dict[str, Any] = instances.get_fields()\n            pred_classes = fields[\"pred_classes\"]  # (n_boxes,)\n            pred_scores = fields[\"scores\"]\n            # shape (n_boxes, 4). (xmin, ymin, xmax, ymax)\n            pred_boxes = fields[\"pred_boxes\"].tensor\n\n            h_ratio = dim0 / resized_height\n            w_ratio = dim1 / resized_width\n            pred_boxes[:, [0, 2]] *= w_ratio\n            pred_boxes[:, [1, 3]] *= h_ratio\n\n            pred_classes_array = pred_classes.cpu().numpy()\n            pred_boxes_array = pred_boxes.cpu().numpy()\n            pred_scores_array = pred_scores.cpu().numpy()\n\n            result = {\n                \"image_id\": image_id,\n                \"PredictionString\": format_pred(\n                    pred_classes_array, pred_boxes_array, pred_scores_array\n                ),\n            }\n        results_list.append(result)\n        index += 1","metadata":{"papermill":{"duration":169.846964,"end_time":"2021-02-06T01:18:09.030345","exception":false,"start_time":"2021-02-06T01:15:19.183381","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T07:44:10.352464Z","iopub.execute_input":"2024-05-24T07:44:10.352756Z","iopub.status.idle":"2024-05-24T07:59:39.044538Z","shell.execute_reply.started":"2024-05-24T07:44:10.352731Z","shell.execute_reply":"2024-05-24T07:59:39.043442Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"cfg.OUTPUT_DIR ./output -> results/v9\nOriginal thresh 0.05\nChanged  thresh 0.15\nCreating data...\nimage shape: (1024, 1024, 3)\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6427/6427 [00:00<00:00, 14672.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Load from cache dataset_dicts_cache_test_debug0.pkl\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1607 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning:\n\ntorch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1607/1607 [15:24<00:00,  1.74it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here I set `cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0` to produce **all the detection box prediction even if confidence score is very low**.<br/>\nActually it affects a lot to score, since competition metric is AP (Average-Precision) which is calculated using the boxes with confidence score = 0~100%.","metadata":{"papermill":{"duration":0.236492,"end_time":"2021-02-06T01:18:09.503575","exception":false,"start_time":"2021-02-06T01:18:09.267083","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# This submission includes only detection model's predictions\nsubmission_det = pd.DataFrame(results_list, columns=['image_id', 'PredictionString'])\n#df = df.rename(columns={'image_id': 'ID', 'PredictionString': 'TARGET'})\nsubmission_det.to_csv(\"/kaggle/working/submission.csv\", index=False)\nsubmission_det","metadata":{"papermill":{"duration":0.885336,"end_time":"2021-02-06T01:18:10.624332","exception":false,"start_time":"2021-02-06T01:18:09.738996","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T08:17:58.160415Z","iopub.execute_input":"2024-05-24T08:17:58.161080Z","iopub.status.idle":"2024-05-24T08:17:58.197198Z","shell.execute_reply.started":"2024-05-24T08:17:58.161044Z","shell.execute_reply":"2024-05-24T08:17:58.196394Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                              image_id PredictionString\n0     j8ucb4pF6s210AWzYbWtWDjkHDfEIvqh   14 1.0 0 0 1 1\n1     llXsB94LSKfiFDTmVqQ8qK5dYtQEyJdN   14 1.0 0 0 1 1\n2     Lvh5LHHZvSPb9N6LVXv1Ez8NMf1XlTki   14 1.0 0 0 1 1\n3     4OPuciU1DBQZJA7kWMFPeCZ58q2bI7w7   14 1.0 0 0 1 1\n4     ZyaXVXMe31nPYUV81ZzE0fe2VAFnhkJu   14 1.0 0 0 1 1\n...                                ...              ...\n6422  iHqtVNSRodeKd1bbFUyjLQnoXpRvzVs8   14 1.0 0 0 1 1\n6423  URqMbGEvFkUabTn3fWS84sUJw7OPbeFF   14 1.0 0 0 1 1\n6424  89jErvxxE1rxBKnDoAYD12exyELelZ1A   14 1.0 0 0 1 1\n6425  pSMAnMlp97Jc2sGxIeQJU81jysvRUDn8   14 1.0 0 0 1 1\n6426  yH7kjXH177YjH4CBImBr8kTr11CLSkMr   14 1.0 0 0 1 1\n\n[6427 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>j8ucb4pF6s210AWzYbWtWDjkHDfEIvqh</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>llXsB94LSKfiFDTmVqQ8qK5dYtQEyJdN</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lvh5LHHZvSPb9N6LVXv1Ez8NMf1XlTki</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4OPuciU1DBQZJA7kWMFPeCZ58q2bI7w7</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ZyaXVXMe31nPYUV81ZzE0fe2VAFnhkJu</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6422</th>\n      <td>iHqtVNSRodeKd1bbFUyjLQnoXpRvzVs8</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6423</th>\n      <td>URqMbGEvFkUabTn3fWS84sUJw7OPbeFF</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6424</th>\n      <td>89jErvxxE1rxBKnDoAYD12exyELelZ1A</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6425</th>\n      <td>pSMAnMlp97Jc2sGxIeQJU81jysvRUDn8</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6426</th>\n      <td>yH7kjXH177YjH4CBImBr8kTr11CLSkMr</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6427 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# MODEL TUNNING","metadata":{}},{"cell_type":"code","source":"!pip install optuna","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:22:10.928237Z","iopub.execute_input":"2024-05-24T14:22:10.928863Z","iopub.status.idle":"2024-05-24T14:22:22.953668Z","shell.execute_reply.started":"2024-05-24T14:22:10.928832Z","shell.execute_reply":"2024-05-24T14:22:22.952560Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (24.0)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport shutil\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2 import model_zoo\nimport optuna\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\n\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\n\ndef convert_csv_to_coco_json(csv_path, img_dir_path, output_json_path):\n    df = pd.read_csv(csv_path)\n\n    images = []\n    annotations = []\n    categories = [{\"id\": i, \"name\": name} for i, name in enumerate([\n        \"Aortic enlargement\", \"Atelectasis\", \"Calcification\", \"Cardiomegaly\", \"Consolidation\", \"ILD\", \"Infiltration\",\n        \"Lung Opacity\", \"Nodule/Mass\", \"Other lesion\", \"Pleural effusion\", \"Pleural thickening\", \"Pneumothorax\",\n        \"Pulmonary fibrosis\", \"No finding\"\n    ])]\n\n    image_ids = df['image_id'].unique()\n    image_id_map = {image_id: idx for idx, image_id in enumerate(image_ids)}\n\n    for image_id in image_ids:\n        img_path = os.path.join(img_dir_path, f\"{image_id}.png\")\n        with Image.open(img_path) as img:\n            width, height = img.size\n        images.append({\n            \"id\": int(image_id_map[image_id]),\n            \"file_name\": f\"{image_id}.png\",\n            \"width\": int(width),\n            \"height\": int(height)\n        })\n\n    for idx, row in df.iterrows():\n        annotations.append({\n            \"id\": int(idx),\n            \"image_id\": int(image_id_map[row['image_id']]),\n            \"category_id\": int(row['class_id']),\n            \"bbox\": [float(row['x_min']), float(row['y_min']), float(row['x_max'] - row['x_min']), float(row['y_max'] - row['y_min'])],\n            \"area\": float((row['x_max'] - row['x_min']) * (row['y_max'] - row['y_min'])),\n            \"iscrowd\": 0\n        })\n\n    coco_format = {\n        \"images\": images,\n        \"annotations\": annotations,\n        \"categories\": categories\n    }\n\n    with open(output_json_path, 'w') as f:\n        json.dump(coco_format, f)\n\n\n# Paths to your files\ntrain_csv_path = \"/kaggle/input/amia-public-challenge-2024/train.csv\"\ntrain_image_dir = \"/kaggle/input/amia-public-challenge-2024/train/train\"\ntest_csv_path = \"/kaggle/input/amia-public-challenge-2024/test.csv\"\ntest_image_dir = \"/kaggle/input/amia-public-challenge-2024/test/test\"\ntrain_json_path = \"/kaggle/working/train_annotations.json\"\ntest_json_path = \"/kaggle/working/test_annotations.json\"\n\n# Convert CSV to COCO JSON\nconvert_csv_to_coco_json(train_csv_path, train_image_dir, train_json_path)\nconvert_csv_to_coco_json(test_csv_path, test_image_dir, test_json_path)\n\n# Register the datasets\nregister_coco_instances(\"lung_train\", {}, train_annotations_path, train_image_dir)\nregister_coco_instances(\"lung_val\", {}, test_annotations_path, test_image_dir)\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Configuration settings\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n    cfg.DATASETS.TRAIN = (\"lung_train\",)\n    cfg.DATASETS.TEST = (\"lung_val\",)  # Validation set\n    cfg.DATALOADER.NUM_WORKERS = trial.suggest_int('num_workers', 2, 8)  # Modify num_workers here\n    \n    # Hyperparameters to tune\n    cfg.SOLVER.BASE_LR = trial.suggest_loguniform('base_lr', 1e-6, 1e-2)\n    cfg.SOLVER.IMS_PER_BATCH = trial.suggest_int('ims_per_batch', 1, 16)\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = trial.suggest_int('batch_size_per_image', 128, 512)\n    cfg.SOLVER.MAX_ITER = trial.suggest_int('max_iter', 100, 1000)\n    \n    # New parameters\n    cfg.SOLVER.CHECKPOINT_PERIOD = trial.suggest_int('eval_period', 500, 2000)  # Checkpoint saving interval\n    cfg.SOLVER.LR_SCHEDULER_NAME = trial.suggest_categorical('lr_scheduler_name', ['WarmupCosineLR', 'WarmupMultiStepLR'])\n    cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = trial.suggest_categorical('use_class14', [True, False])\n    \n    # Other configurations\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 14 if not cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS else 15  # 14 classes or 15 if including \"No finding\"\n    cfg.OUTPUT_DIR = \"./output\"\n    \n    # Trainer\n    trainer = DefaultTrainer(cfg)\n    trainer.resume_or_load(resume=False)\n    trainer.train()\n    \n    # Save the model\n    checkpointer = DetectionCheckpointer(trainer.model)\n    checkpointer.save(\"model_final\")\n    \n    # Evaluator\n    evaluator = COCOEvaluator(\"lung_val\", cfg, False, output_dir=\"./output/\")\n    val_loader = build_detection_test_loader(cfg, \"lung_val\")\n    metrics = inference_on_dataset(trainer.model, val_loader, evaluator)\n    \n    return metrics[\"bbox\"][\"AP\"]  # Use the Average Precision metric\n\n# Create and optimize the study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\n# Save the best trial's hyperparameters\nbest_trial = study.best_trial\nbest_params = best_trial.params\ntorch.save(best_params, \"tunned_params_v2.pth\")\n\n# Save the model to a file\nfinal_model_path = \"/kaggle/working/model_tunning_v2.pth\"\ncheckpointer.save(final_model_path)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport shutil\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2 import model_zoo\nimport optuna\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\n\nimport json\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\ndef convert_csv_to_coco_json(csv_path, img_dir_path, output_json_path):\n    df = pd.read_csv(csv_path)\n    \n    # Debugging step to print columns\n    print(\"Columns in CSV:\", df.columns)\n\n    images = []\n    annotations = []\n    categories = [{\"id\": i, \"name\": name} for i, name in enumerate([\n        \"Aortic enlargement\", \"Atelectasis\", \"Calcification\", \"Cardiomegaly\", \"Consolidation\", \"ILD\", \"Infiltration\",\n        \"Lung Opacity\", \"Nodule/Mass\", \"Other lesion\", \"Pleural effusion\", \"Pleural thickening\", \"Pneumothorax\",\n        \"Pulmonary fibrosis\", \"No finding\"\n    ])]\n\n    image_ids = df['image_id'].unique()\n    image_id_map = {image_id: idx for idx, image_id in enumerate(image_ids)}\n\n    for image_id in image_ids:\n        img_path = os.path.join(img_dir_path, f\"{image_id}.png\")\n        with Image.open(img_path) as img:\n            width, height = img.size\n        images.append({\n            \"id\": int(image_id_map[image_id]),\n            \"file_name\": f\"{image_id}.png\",\n            \"width\": int(width),\n            \"height\": int(height)\n        })\n\n    for idx, row in df.iterrows():\n        annotations.append({\n            \"id\": int(idx),\n            \"image_id\": int(image_id_map[row['image_id']]),\n            \"category_id\": int(row['class_id']),\n            \"bbox\": [float(row['x_min']), float(row['y_min']), float(row['x_max'] - row['x_min']), float(row['y_max'] - row['y_min'])],\n            \"area\": float((row['x_max'] - row['x_min']) * (row['y_max'] - row['y_min'])),\n            \"iscrowd\": 0\n        })\n\n    coco_format = {\n        \"images\": images,\n        \"annotations\": annotations,\n        \"categories\": categories\n    }\n\n    with open(output_json_path, 'w') as f:\n        json.dump(coco_format, f)\n\n# Paths to your files\ntrain_csv_path = \"/kaggle/input/amia-public-challenge-2024/train.csv\"\ntrain_image_dir = \"/kaggle/input/amia-public-challenge-2024/train/train\"\ntest_csv_path = \"/kaggle/input/amia-public-challenge-2024/test.csv\"\ntest_image_dir = \"/kaggle/input/amia-public-challenge-2024/test/test\"\ntrain_json_path = \"/kaggle/working/train_annotations.json\"\ntest_json_path = \"/kaggle/working/test_annotations.json\"\n\n# Convert CSV to COCO JSON\nconvert_csv_to_coco_json(train_csv_path, train_image_dir, train_json_path)\nconvert_csv_to_coco_json(test_csv_path, test_image_dir, test_json_path)\n\n# Register the datasets\nregister_coco_instances(\"lung_train\", {}, train_json_path, train_image_dir)\nregister_coco_instances(\"lung_val\", {}, test_json_path, test_image_dir)\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Configuration settings\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n    cfg.DATASETS.TRAIN = (\"lung_train\",)\n    cfg.DATASETS.TEST = ()  # Validation set\n    cfg.DATALOADER.NUM_WORKERS = trial.suggest_int('num_workers', 2, 8)  # Modify num_workers here\n    \n    # Hyperparameters to tune\n    cfg.SOLVER.BASE_LR = trial.suggest_loguniform('base_lr', 1e-6, 1e-2)\n    cfg.SOLVER.IMS_PER_BATCH = trial.suggest_int('ims_per_batch', 1, 16)\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = trial.suggest_int('batch_size_per_image', 128, 512)\n    cfg.SOLVER.MAX_ITER = trial.suggest_int('max_iter', 100, 1000)\n    \n    # New parameters\n    cfg.SOLVER.CHECKPOINT_PERIOD = trial.suggest_int('eval_period', 500, 2000)  # Checkpoint saving interval\n    cfg.SOLVER.LR_SCHEDULER_NAME = trial.suggest_categorical('lr_scheduler_name', ['WarmupCosineLR', 'WarmupMultiStepLR'])\n    cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = trial.suggest_categorical('use_class14', [True, False])\n    \n    # Other configurations\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 14 if not cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS else 15  # 14 classes or 15 if including \"No finding\"\n    cfg.OUTPUT_DIR = \"./output\"\n    \n    # Trainer\n    trainer = DefaultTrainer(cfg)\n    trainer.resume_or_load(resume=False)\n    trainer.train()\n    \n    # Save the model\n    checkpointer = DetectionCheckpointer(trainer.model)\n    checkpointer.save(\"model_final\")\n    \n    # Evaluator\n    evaluator = COCOEvaluator(\"lung_val\", cfg, False, output_dir=\"./output/\")\n    val_loader = build_detection_test_loader(cfg, \"lung_val\")\n    metrics = inference_on_dataset(trainer.model, val_loader, evaluator)\n    \n    return metrics[\"bbox\"][\"AP\"]  # Use the Average Precision metric\n\n# Create and optimize the study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\n# Save the best trial's hyperparameters\nbest_trial = study.best_trial\nbest_params = best_trial.params\ntorch.save(best_params, \"tunned_params_v2.pth\")\n\n# Save the model to a file\nfinal_model_path = \"/kaggle/working/model_tunning_v2.pth\"\ncheckpointer.save(final_model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:25:56.902996Z","iopub.status.idle":"2024-05-24T14:25:56.903374Z","shell.execute_reply.started":"2024-05-24T14:25:56.903204Z","shell.execute_reply":"2024-05-24T14:25:56.903219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the best hyperparameters\nprint(\"Best hyperparameters: \", study.best_params)\nprint(\"Best AP: \", study.best_value)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T08:33:50.825836Z","iopub.status.idle":"2024-05-24T08:33:50.826274Z","shell.execute_reply.started":"2024-05-24T08:33:50.826051Z","shell.execute_reply":"2024-05-24T08:33:50.826069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the final model using the best hyperparameters\nbest_params = study.best_params\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\ncfg.SOLVER.BASE_LR = best_params['base_lr']\ncfg.SOLVER.IMS_PER_BATCH = best_params['ims_per_batch']\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = best_params['batch_size_per_image']\ncfg.SOLVER.MAX_ITER = best_params['max_iter']\n\ncfg.DATASETS.TRAIN = (\"lung_train3\",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 15\n\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:38:27.575592Z","iopub.execute_input":"2024-05-23T19:38:27.576318Z","iopub.status.idle":"2024-05-23T20:12:31.715232Z","shell.execute_reply.started":"2024-05-23T19:38:27.576286Z","shell.execute_reply":"2024-05-23T20:12:31.713777Z"},"trusted":true},"execution_count":19,"outputs":[]}]}